{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ed2a69-cf16-4ea5-bee1-1d90a3870f22",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f3d7f7-f6a2-464c-9882-03008171727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import IsolationForest, HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import f\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a20b66-c4f9-41cc-b0b3-3724c0b73523",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e839897-c7d7-42e4-ad55-e69157797ec5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ad22dc-a5e0-43c1-98bc-702ccb1772ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(header_lines):\n",
    "    \"\"\"\n",
    "    Extracts offset and postseismic decay information from the header lines of a GNSS file.\n",
    "\n",
    "    The function captures both coseismic and non-coseismic offsets, along with postseismic decays, for \n",
    "    north (N), east (E), and up (U) components. It parses lines starting with '#' and collects the relevant \n",
    "    values into a structured dictionary categorized by the component.\n",
    "\n",
    "    Parameters:\n",
    "    - header_lines (list of str): Lines from the file that contain metadata and comments starting with '#'.\n",
    "\n",
    "    Returns:\n",
    "    - components (dict): A dictionary with keys 'n', 'e', and 'u' representing the north, east, and up components.\n",
    "      Each component holds a dictionary with:\n",
    "        - 'offsets': A list of dictionaries containing offset information (value, error, date, coseismic flag).\n",
    "        - 'ps_decays': A list of dictionaries containing postseismic decay information (value, error, tau, date, type).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Capture important information from the header\n",
    "    offset_pattern = re.compile(r\"#\\s*(\\*?)\\s*offset\\s+\\d+:?\\s+([-\\d.]+)\\s+\\+/\\-\\s+([-\\d.]+)\\s+mm.*?\\((\\d{4}-\\d{2}-\\d{2}).*?\\)\")\n",
    "    ps_decay_pattern = re.compile(r'#!?\\s*ps decay\\s+\\d:\\s*(-?\\d+\\.\\d+)\\s+\\+/-\\s+(\\d+\\.\\d+)\\s+mm\\s+\\((\\d{4}-\\d{2}-\\d{2})\\s+\\[(\\d{4}\\.\\d+)\\]\\);\\s*tau:\\s*(\\d+)\\s+days')\n",
    "    component_pattern = re.compile(r\"#\\s+([neu])\\s+component\")\n",
    "\n",
    "    components = {'n': {'offsets': [], 'ps_decays': []}, 'e': {'offsets': [], 'ps_decays': []}, 'u': {'offsets': [], 'ps_decays': []}}\n",
    "    current_component = None\n",
    "\n",
    "    for line in header_lines:\n",
    "        comp_match = component_pattern.match(line)\n",
    "        if comp_match:\n",
    "            current_component = comp_match.group(1)\n",
    "            continue\n",
    "\n",
    "        # Check for offset\n",
    "        offset_match = offset_pattern.match(line)\n",
    "        if offset_match and current_component:\n",
    "            coseismic = bool(offset_match.group(1))  # True if * present, meaning coseismic\n",
    "            offset_value = float(offset_match.group(2))\n",
    "            offset_error = float(offset_match.group(3))\n",
    "            offset_date = offset_match.group(4)\n",
    "            components[current_component]['offsets'].append({\n",
    "                'value': offset_value,\n",
    "                'error': offset_error,\n",
    "                'date': offset_date,\n",
    "                'coseismic': coseismic\n",
    "            })\n",
    "\n",
    "        # Check for postseismic decay\n",
    "        ps_decay_match = ps_decay_pattern.match(line)\n",
    "        if ps_decay_match and current_component:\n",
    "            decay_value = float(ps_decay_match.group(1))\n",
    "            decay_error = float(ps_decay_match.group(2))\n",
    "            decay_date = ps_decay_match.group(3)\n",
    "            tau = int(ps_decay_match.group(5))\n",
    "            # Determine decay type based on the presence of '!'\n",
    "            decay_type = 'logarithmic' if '!' in line else 'exponential'\n",
    "            components[current_component]['ps_decays'].append({\n",
    "                'value': decay_value,\n",
    "                'error': decay_error,\n",
    "                'tau': tau,\n",
    "                'date': decay_date,\n",
    "                'type': decay_type\n",
    "            })\n",
    "\n",
    "    return components\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reads a GNSS file, extracting both header and data information into a pandas DataFrame.\n",
    "\n",
    "    The function processes the header to extract metadata (e.g., station coordinates, height, offsets, decays) \n",
    "    and processes the data section to extract time-series GNSS measurements. It combines these into a DataFrame \n",
    "    with attributes containing additional metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the file containing GNSS data.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame): A DataFrame containing the time-series GNSS data (N, E, U components, sigmas, correlations),\n",
    "      indexed by date. The DataFrame has additional attributes storing station geometry (latitude, longitude), height, \n",
    "      and offset/decay information.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(DATA_DIR+filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    header_lines = [line for line in lines if line.startswith('#')]\n",
    "    if header_lines:\n",
    "        column_names = re.split(r'\\s{2,}', header_lines[-1].lstrip('#').strip())\n",
    "    else:\n",
    "        column_names = []\n",
    "        \n",
    "    data_lines = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.strip().split()\n",
    "            # Check if the number of parts matches the expected number of columns\n",
    "            if len(parts) < len(column_names):\n",
    "                # Add None for missing values\n",
    "                parts.extend([None] * (len(column_names) - len(parts)))\n",
    "            data_lines.append(parts)\n",
    "\n",
    "    data = pd.DataFrame(data_lines)\n",
    "    data.columns = column_names\n",
    "    \n",
    "    # Extracts latitude, longitude and height\n",
    "    pattern = r'Latitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|East Longitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|Height\\s*\\(M\\)\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    #referece_pattern = r'Reference_X\\s*:\\s*(-?\\d+\\.\\d+)|Reference_Y\\s*:\\s*(-?\\d+\\.\\d+)|Reference_Z\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    matches = re.findall(pattern, ' '.join(header_lines))\n",
    "    geom = Point(float(matches[1][1]), float(matches[0][0]))\n",
    "    \n",
    "    offsets = get_offsets(header_lines)\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Yr'].astype(str) + data['DayOfYr'].astype(str), format='%Y%j')\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data.drop(['Dec Yr', 'Yr', 'DayOfYr', 'Chi-Squared'], axis=1, inplace=True)\n",
    "    cols = ['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']\n",
    "    data[cols] = data[cols].astype(float)\n",
    "    \n",
    "    data.name = filename.replace(\"RawTrend.neu\", \"\")\n",
    "    data.attrs['geometry'] = geom\n",
    "    data.attrs['height'] = float(matches[2][2])\n",
    "    data.attrs['offsets'] = offsets\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15684cb8-c2dd-4522-8a89-ec2929952c82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c71a25a-f9a0-406b-a6d1-8d315f638289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_dates(df):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df_full = df.reindex(full_date_range)\n",
    "    df_full.name = df.name\n",
    "    return df_full\n",
    "\n",
    "def clean_dataframes(dfs, missing_value_threshold=None):\n",
    "    \"\"\"\n",
    "    Cleans the dataframes by:\n",
    "    1. Removing dataframes without any coseismic offsets in any of the 3 components (n, e, u).\n",
    "    2. Removing non-coseismic offsets from all components.\n",
    "    3. Optionally removing dataframes with excessive missing values in all 3 components.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "    missing_value_threshold (float, optional): Percentage (0 to 1) of allowed missing values.\n",
    "                                               If exceeded, the dataframe is removed.\n",
    "\n",
    "    Returns:\n",
    "    list: Cleaned list of dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_dfs = []\n",
    "    components = ['N', 'E', 'U']\n",
    "    components_offsets = ['n', 'e', 'u']\n",
    "\n",
    "    for org_df in dfs:\n",
    "        \n",
    "        has_coseismic = False\n",
    "        df = add_missing_dates(org_df)\n",
    "\n",
    "        for comp in components_offsets:\n",
    "            filtered_offsets = []\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                if offset['coseismic']:\n",
    "                    has_coseismic = True\n",
    "                    filtered_offsets.append(offset)\n",
    "            # Update offsets to retain only coseismic\n",
    "            df.attrs['offsets'][comp]['offsets'] = filtered_offsets\n",
    "\n",
    "        # Skip dataframe if no coseismic offsets in any component\n",
    "        if not has_coseismic:\n",
    "            continue\n",
    "\n",
    "        # Check missing values for all components combined, if threshold is provided\n",
    "        if missing_value_threshold is not None:\n",
    "            total_values = sum(df[comp].size for comp in components)\n",
    "            missing_values = sum(df[comp].isna().sum() for comp in components)\n",
    "\n",
    "            missing_percentage = missing_values / total_values\n",
    "            if missing_percentage > missing_value_threshold:\n",
    "                continue  # Skip the dataframe if missing values exceed the threshold\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        cleaned_dfs.append(df)\n",
    "\n",
    "    return cleaned_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91040101-1795-4f46-ba2c-0adc9b1f9fc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chow test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca7c454-83c8-4c61-a8c8-101ffd56fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chow_test(df, alpha=0.01, treshold=1, direction='N'):\n",
    "\n",
    "    chow_df = pd.DataFrame(index=df.index, columns=['break', 'score'], dtype=float)\n",
    "    chow_df['score'] = np.nan  # Initialize all 'score' values to NaN\n",
    "    chow_df['break'] = 0  # Initialize all 'break' values to 0\n",
    "    \n",
    "    n = len(df)\n",
    "    window_N = 60\n",
    "    k = 2\n",
    "    \n",
    "    for i in range(n - window_N + 1):\n",
    "        # Define windows\n",
    "        full_window = df.iloc[i:i+window_N]\n",
    "        left = full_window.iloc[:window_N//2]\n",
    "        right = full_window.iloc[window_N//2:]\n",
    "\n",
    "        # Full window\n",
    "        y_window = full_window[direction].values\n",
    "        coef_window = np.polyfit(np.arange(window_N), y_window, 1)\n",
    "        pred_window = np.polyval(coef_window, np.arange(window_N))\n",
    "        Sc = np.sum((y_window - pred_window) ** 2)\n",
    "\n",
    "        # Left window\n",
    "        y_left = left[direction].values\n",
    "        coef_left = np.polyfit(np.arange(len(left)), y_left, 1)\n",
    "        pred_left = np.polyval(coef_left, np.arange(len(left)))\n",
    "        S1 = np.sum((y_left - pred_left) ** 2)\n",
    "        N1 = len(left)\n",
    "\n",
    "        # Right window\n",
    "        y_right = right[direction].values\n",
    "        coef_right = np.polyfit(np.arange(len(right)), y_right, 1)\n",
    "        pred_right = np.polyval(coef_right, np.arange(len(right)))\n",
    "        S2 = np.sum((y_right - pred_right) ** 2)\n",
    "        N2 = len(right)\n",
    "        \n",
    "        # Calculate CTS\n",
    "        cts = ((Sc - (S1 + S2)) / k ) / ((S1 + S2) / (N1 + N2 - 2 * k))\n",
    "        dfd = N1 + N2 - 2 * k\n",
    "        c_value = f.ppf(q=1-alpha, dfn=k, dfd=dfd)\n",
    "        score = cts / c_value\n",
    "\n",
    "\n",
    "        chow_df.at[full_window.index[window_N//2], 'score'] = cts \n",
    "        if abs(score) > treshold:\n",
    "            chow_df.at[full_window.index[window_N//2], 'break'] = 1\n",
    "\n",
    "    return chow_df\n",
    "\n",
    "def apply_chow_test(stations_list, alpha=0.01, treshold=1):\n",
    "    result_list = []\n",
    "\n",
    "    for idx, station_df in enumerate(stations_list):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing station {idx}...\")\n",
    "            \n",
    "        station_result = {'station_idx': idx}\n",
    "        \n",
    "        # Apply chow test to each direction (N, E, U)\n",
    "        for direction in ['N', 'E', 'U']:\n",
    "            result_key = f'breaks_{direction.lower()}'\n",
    "            station_result[result_key] = chow_test(station_df[[direction]], alpha=alpha, treshold=treshold, direction=direction)\n",
    "        \n",
    "        result_list.append(station_result)\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2728c2a-7eec-47ef-8407-9282bcd11c3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd32437c-f9f4-48ff-a8a4-f1eaaeebd2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_in_range(break_date_idx, offset_dates, threshold_days=30):\n",
    "    \"\"\"Check if a detected break (by its index) is within a certain range of any known offsets (within threshold days).\"\"\"\n",
    "    for offset_date in offset_dates:\n",
    "        if abs((break_date_idx - offset_date).days) <= threshold_days:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_breaks(result_list, stations_list, threshold_days=30):\n",
    "    stats = {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0}\n",
    "\n",
    "    # Loop through each station and compare breaks with offsets\n",
    "    for idx, station_result in enumerate(result_list):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing station {idx}...\")\n",
    "            \n",
    "        station = stations_list[idx]\n",
    "        offsets = station.attrs['offsets']\n",
    "\n",
    "        for direction in ['n', 'e', 'u']:\n",
    "            result_key = f'breaks_{direction}'\n",
    "            breaks_df = station_result[result_key]\n",
    "\n",
    "            # Get list of known offsets for this direction and convert them to datetime objects\n",
    "            known_offsets = offsets[direction]['offsets']\n",
    "            offset_dates = [datetime.strptime(offset['date'], '%Y-%m-%d') for offset in known_offsets]\n",
    "            used_offsets = set()  # Set to track used offsets in each direction\n",
    "\n",
    "            # True positives and false positives\n",
    "            detected_breaks = breaks_df[breaks_df['break'] == 1]\n",
    "            for break_idx, _ in detected_breaks.iterrows():\n",
    "                break_date_idx = pd.to_datetime(break_idx)  # Ensure index is a Timestamp\n",
    "                \n",
    "                if date_in_range(break_date_idx, offset_dates, threshold_days):\n",
    "                    if break_date_idx not in used_offsets:  # Count offset only once per direction\n",
    "                        stats['TP'] += 1  # True positive\n",
    "                        used_offsets.add(break_date_idx)\n",
    "                else:\n",
    "                    stats['FP'] += 1  # False positive\n",
    "\n",
    "            # False negatives: unmatched offsets in each direction\n",
    "            for offset in known_offsets:\n",
    "                offset_date = datetime.strptime(offset['date'], '%Y-%m-%d')\n",
    "                if offset_date not in used_offsets and not date_in_range(offset_date, detected_breaks.index, threshold_days):\n",
    "                    stats['FN'] += 1  # False negative\n",
    "\n",
    "            # True negatives\n",
    "            non_breaks = breaks_df[breaks_df['break'] == 0]\n",
    "            for non_break_idx, _ in non_breaks.iterrows():\n",
    "                non_break_date_idx = pd.to_datetime(non_break_idx)  # Ensure index is a Timestamp\n",
    "                \n",
    "                if not date_in_range(non_break_date_idx, offset_dates, threshold_days):\n",
    "                    stats['TN'] += 1  # True negative\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6786d-bc6c-4924-9972-541d1a5f5d05",
   "metadata": {},
   "source": [
    "## Calling the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13cd8d-f50e-4592-ac95-3c630927a7d7",
   "metadata": {},
   "source": [
    "### Choose the treshold for the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddc4e8f5-1eef-4258-916b-bfce6daa1252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Adjust name\\nwith open('storage/cleaned_dfs.pkl', 'rb') as file:\\n    cleaned_dfs = pickle.load(file)\\n    \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "dir = Path(DATA_DIR)\n",
    "for file_path in dir.iterdir():\n",
    "    if file_path.is_file():\n",
    "        dfs.append(read_file(file_path.name))\n",
    "\n",
    "# Adjust treshold\n",
    "cleaned_dfs = clean_dataframes(dfs, missing_value_threshold=0.05)\n",
    "print(len(cleaned_dfs))\n",
    "\n",
    "#Adjust path and name\n",
    "save_dir = 'Storage/5_percent'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, 'cleaned_dfs.pkl'), 'wb') as file:\n",
    "    pickle.dump(cleaned_dfs, file)\n",
    "\n",
    "\"\"\"\n",
    "#Adjust name\n",
    "with open(os.path.join(save_dir, 'cleaned_dfs.pkl'), 'rb') as file:\n",
    "    cleaned_dfs = pickle.load(file)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0064b-4a45-4d0c-a4fc-3ae2f6f4eb45",
   "metadata": {},
   "source": [
    "### Choose the treshold for the chow test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbba55f-645a-4357-8260-100d94bad9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station 0...\n"
     ]
    }
   ],
   "source": [
    "# Adjust treshold\n",
    "results = apply_chow_test(cleaned_dfs, alpha=0.1, treshold=15)\n",
    "\n",
    "#Adjust name\n",
    "with open(os.path.join(save_dir, 'results15_a0_1.pkl'), 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "\n",
    "\"\"\"\n",
    "#Adjust name\n",
    "with open(os.path.join(save_dir, 'results.pkl'), 'rb') as file:\n",
    "    results = pickle.load(file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42dd77-5a08-4c7a-ad34-95bb67d88a47",
   "metadata": {},
   "source": [
    "### Chose the treshold for the number of days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c86c4039-08e7-4a9a-8874-8deb8b08bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station 0...\n",
      "Processing station 100...\n",
      "Processing station 200...\n",
      "Processing station 300...\n",
      "Processing station 400...\n",
      "Processing station 500...\n",
      "Processing station 600...\n",
      "True Positives: 351\n",
      "False Positives: 604\n",
      "False Negatives: 2321\n",
      "True Negatives: 13790016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Adjust name\\nstats_df = pd.read_csv('stats.csv')\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust treshold\n",
    "stats = evaluate_breaks(results, cleaned_dfs, threshold_days=0)\n",
    "\n",
    "print(f\"True Positives: {stats['TP']}\")\n",
    "print(f\"False Positives: {stats['FP']}\")\n",
    "print(f\"False Negatives: {stats['FN']}\")\n",
    "print(f\"True Negatives: {stats['TN']}\")\n",
    "stats_df = pd.DataFrame([stats])\n",
    "\n",
    "#Adjust name\n",
    "stats_df.to_csv(os.path.join(save_dir, 'stats20_0_a0_1.csv'), index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Adjust name\n",
    "stats_df = pd.read_csv('stats.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb5da6-e185-4118-bd71-72e4e010d842",
   "metadata": {},
   "source": [
    "Old version: If multiple breaks are detected within a close window around a single true offset, the sum of true positives (TP) and false negatives (FN) may increase beyond the actual number of true offsets.\n",
    "\n",
    "Now: When you increase the threshold, you're expanding the range around each offset in which a predicted break can be considered a true positive (TP).\n",
    "If there is a break predicted within the threshold window of an offset, it will now count as a true positive, whereas before it might have been considered a false positive (FP) or a false negative (FN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a4de5-d2bf-4144-afb5-824e0f92977f",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e375ae9-9146-412f-8fd0-b63be37df94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(tp, fp, fn, tn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94f09a7a-e600-41f4-ab08-1ace55efaa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 10, Days window: 0\n",
      "Precision: 0.0494\n",
      "Recall: 0.2242\n",
      "F1-Score: 0.0809\n",
      "Accuracy: 0.9990\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 15, Days window: 0\n",
      "Precision: 0.1460\n",
      "Recall: 0.1755\n",
      "F1-Score: 0.1594\n",
      "Accuracy: 0.9996\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 20, Days window: 0\n",
      "Precision: 0.2695\n",
      "Recall: 0.1471\n",
      "F1-Score: 0.1903\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 20, Days window: 0\n",
      "Precision: 0.3675\n",
      "Recall: 0.1314\n",
      "F1-Score: 0.1935\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 20, Days window: 1\n",
      "Precision: 0.4232\n",
      "Recall: 0.2148\n",
      "F1-Score: 0.2849\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 20, Days window: 5\n",
      "Precision: 0.4287\n",
      "Recall: 0.2170\n",
      "F1-Score: 0.2882\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 20, Days window: 10\n",
      "Precision: 0.4383\n",
      "Recall: 0.2208\n",
      "F1-Score: 0.2937\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 20, Days window: 30\n",
      "Precision: 0.4705\n",
      "Recall: 0.2337\n",
      "F1-Score: 0.3122\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 25, Days window: 0\n",
      "Precision: 0.3948\n",
      "Recall: 0.1250\n",
      "F1-Score: 0.1899\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n",
      "Experiment Parameters:\n",
      "Outliers: <5%, Chow Treshold: 50, Days window: 0\n",
      "Precision: 0.7329\n",
      "Recall: 0.0801\n",
      "F1-Score: 0.1444\n",
      "Accuracy: 0.9998\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "outliers_percentage = save_dir.split('/')[-1].replace('_percent', '%')\n",
    "\n",
    "# Get all 'stats' csv files in the directory\n",
    "files = [f for f in os.listdir(save_dir) if f.startswith('stats') and f.endswith('.csv')]\n",
    "\n",
    "# Extract chow threshold and day window from file names and store along with the file\n",
    "file_info = []\n",
    "for file in files:\n",
    "    file_name_parts = file.replace('.csv', '').replace('stats', '').split('_')\n",
    "    chow_treshold = int(file_name_parts[0])\n",
    "    days_window = int(file_name_parts[1])\n",
    "    file_info.append((chow_treshold, days_window, file))\n",
    "\n",
    "# Sort the files by chow threshold first, and then by day window\n",
    "file_info_sorted = sorted(file_info, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "# Iterate over each file (sorted) and process\n",
    "for chow_treshold, days_window, file in file_info_sorted:\n",
    "    file_path = os.path.join(save_dir, file)\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Assume the csv has columns ['TP', 'FP', 'FN', 'TN']\n",
    "    tp = df['TP'].sum()\n",
    "    fp = df['FP'].sum()\n",
    "    fn = df['FN'].sum()\n",
    "    tn = df['TN'].sum()\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, accuracy = calculate_metrics(tp, fp, fn, tn)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Experiment Parameters:\")\n",
    "    print(f\"Outliers: <{outliers_percentage}, Chow Treshold: {chow_treshold}, Days window: {days_window}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"-\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed158cd2-e863-4895-9f28-5fd98d9fa8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of offsets found in cleaned_dfs: 2672\n"
     ]
    }
   ],
   "source": [
    "def count_total_offsets(stations_list):\n",
    "    total_offsets = 0\n",
    "    \n",
    "    for station_df in stations_list:\n",
    "        # Access the offsets from the station attributes\n",
    "        offsets = station_df.attrs.get('offsets', {})\n",
    "        \n",
    "        # Sum the offsets for all directions (n, e, u)\n",
    "        for direction in offsets.values():\n",
    "            # Each direction has a list of offsets\n",
    "            total_offsets += len(direction['offsets'])  # Count the number of offsets in this direction\n",
    "\n",
    "    return total_offsets\n",
    "\n",
    "total_offsets = count_total_offsets(cleaned_dfs)\n",
    "print(f\"Total number of offsets found in cleaned_dfs: {total_offsets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aaf46-e672-4fcc-bd1e-66ca3b8e1433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
