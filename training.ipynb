{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import IsolationForest, HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DATA_PATH = '/cluster/home/nteutschm/eqdetection/data/'\n",
    "RANDOM_STATE = 86\n",
    "MODEL_TYPE = 'RandomForest' # IsolationForest HistGradientBoosting RandomForest\n",
    "MODEL_PATH = f'/cluster/scratch/nteutschm/eqdetection/models/{MODEL_TYPE}.pkl'\n",
    "PREDICTIONS_PATH = f'/cluster/scratch/nteutschm/eqdetection/predictions/{MODEL_TYPE}.csv'\n",
    "FEATURES_PATH = f'/cluster/scratch/nteutschm/eqdetection/features/{MODEL_TYPE}'\n",
    "\n",
    "LOAD_MODEL = False # If already trained model is saved under MODEL_PATH, it can be loaded if set to True to skip the entire training process\n",
    "\n",
    "# Optimal parameters:\n",
    "OPTIMAL_PARAMS = False # If optimal parametrs should be used, or the parameters should be tuned (set to False)\n",
    "\n",
    "# If OPTIMAL_PARAMS is True, these parameters are used for the training process:\n",
    "BEST_PARAMS_RANDOM_FOREST = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 30,\n",
    "    'class_weight': {0: 0.5520685260526444, 1: 5.3013650270651915},\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "BEST_PARAMS_ISOLATION_FOREST = {\n",
    "    'n_estimators': 300,\n",
    "    'max_samples': 0.8,\n",
    "    'contamination': 0.001,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "BEST_PARAMS_HIST_GRADIENT_BOOSTING = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_iter': 200,\n",
    "    'max_depth': 10,\n",
    "    'class_weight': {0: 0.5520685260526444, 1: 5.3013650270651915},\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'early_stopping': True,\n",
    "    'n_iter_no_change': 7,\n",
    "    'validation_fraction': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(header_lines):\n",
    "    \"\"\"\n",
    "    Extracts offset and postseismic decay information from the header lines of a GNSS file.\n",
    "\n",
    "    The function captures both coseismic and non-coseismic offsets, along with postseismic decays, for \n",
    "    north (N), east (E), and up (U) components. It parses lines starting with '#' and collects the relevant \n",
    "    values into a structured dictionary categorized by the component.\n",
    "\n",
    "    Parameters:\n",
    "    - header_lines (list of str): Lines from the file that contain metadata and comments starting with '#'.\n",
    "\n",
    "    Returns:\n",
    "    - components (dict): A dictionary with keys 'n', 'e', and 'u' representing the north, east, and up components.\n",
    "      Each component holds a dictionary with:\n",
    "        - 'offsets': A list of dictionaries containing offset information (value, error, date, coseismic flag).\n",
    "        - 'ps_decays': A list of dictionaries containing postseismic decay information (value, error, tau, date, type).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Capture important information from the header\n",
    "    offset_pattern = re.compile(r\"#\\s*(\\*?)\\s*offset\\s+\\d+:?\\s+([-\\d.]+)\\s+\\+/\\-\\s+([-\\d.]+)\\s+mm.*?\\((\\d{4}-\\d{2}-\\d{2}).*?\\)\")\n",
    "    ps_decay_pattern = re.compile(r'#!?\\s*ps decay\\s+\\d:\\s*(-?\\d+\\.\\d+)\\s+\\+/-\\s+(\\d+\\.\\d+)\\s+mm\\s+\\((\\d{4}-\\d{2}-\\d{2})\\s+\\[(\\d{4}\\.\\d+)\\]\\);\\s*tau:\\s*(\\d+)\\s+days')\n",
    "    component_pattern = re.compile(r\"#\\s+([neu])\\s+component\")\n",
    "\n",
    "    components = {'n': {'offsets': [], 'ps_decays': []}, 'e': {'offsets': [], 'ps_decays': []}, 'u': {'offsets': [], 'ps_decays': []}}\n",
    "    current_component = None\n",
    "\n",
    "    for line in header_lines:\n",
    "        comp_match = component_pattern.match(line)\n",
    "        if comp_match:\n",
    "            current_component = comp_match.group(1)\n",
    "            continue\n",
    "\n",
    "        # Check for offset\n",
    "        offset_match = offset_pattern.match(line)\n",
    "        if offset_match and current_component:\n",
    "            coseismic = bool(offset_match.group(1))  # True if * present, meaning coseismic\n",
    "            offset_value = float(offset_match.group(2))\n",
    "            offset_error = float(offset_match.group(3))\n",
    "            offset_date = offset_match.group(4)\n",
    "            components[current_component]['offsets'].append({\n",
    "                'value': offset_value,\n",
    "                'error': offset_error,\n",
    "                'date': offset_date,\n",
    "                'coseismic': coseismic\n",
    "            })\n",
    "\n",
    "        # Check for postseismic decay\n",
    "        ps_decay_match = ps_decay_pattern.match(line)\n",
    "        if ps_decay_match and current_component:\n",
    "            decay_value = float(ps_decay_match.group(1))\n",
    "            decay_error = float(ps_decay_match.group(2))\n",
    "            decay_date = ps_decay_match.group(3)\n",
    "            tau = int(ps_decay_match.group(5))\n",
    "            # Determine decay type based on the presence of '!'\n",
    "            decay_type = 'logarithmic' if '!' in line else 'exponential'\n",
    "            components[current_component]['ps_decays'].append({\n",
    "                'value': decay_value,\n",
    "                'error': decay_error,\n",
    "                'tau': tau,\n",
    "                'date': decay_date,\n",
    "                'type': decay_type\n",
    "            })\n",
    "\n",
    "    return components\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reads a GNSS file, extracting both header and data information into a pandas DataFrame.\n",
    "\n",
    "    The function processes the header to extract metadata (e.g., station coordinates, height, offsets, decays) \n",
    "    and processes the data section to extract time-series GNSS measurements. It combines these into a DataFrame \n",
    "    with attributes containing additional metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the file containing GNSS data.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame): A DataFrame containing the time-series GNSS data (N, E, U components, sigmas, correlations),\n",
    "      indexed by date. The DataFrame has additional attributes storing station geometry (latitude, longitude), height, \n",
    "      and offset/decay information.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(DATA_PATH+filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    header_lines = [line for line in lines if line.startswith('#')]\n",
    "    if header_lines:\n",
    "        column_names = re.split(r'\\s{2,}', header_lines[-1].lstrip('#').strip())\n",
    "    else:\n",
    "        column_names = []\n",
    "        \n",
    "    data_lines = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.strip().split()\n",
    "            # Check if the number of parts matches the expected number of columns\n",
    "            if len(parts) < len(column_names):\n",
    "                # Add None for missing values\n",
    "                parts.extend([None] * (len(column_names) - len(parts)))\n",
    "            data_lines.append(parts)\n",
    "\n",
    "    data = pd.DataFrame(data_lines)\n",
    "    data.columns = column_names\n",
    "    \n",
    "    # Extracts latitude, longitude and height\n",
    "    pattern = r'Latitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|East Longitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|Height\\s*\\(M\\)\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    matches = re.findall(pattern, ' '.join(header_lines))\n",
    "    geom = Point(float(matches[1][1]), float(matches[0][0]))\n",
    "    \n",
    "    offsets = get_offsets(header_lines)\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Yr'].astype(str) + data['DayOfYr'].astype(str), format='%Y%j')\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data.drop(['Dec Yr', 'Yr', 'DayOfYr', 'Chi-Squared'], axis=1, inplace=True)\n",
    "    cols = ['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']\n",
    "    data[cols] = data[cols].astype(float)\n",
    "    \n",
    "    data.name = filename.replace(\"RawTrend.neu\", \"\")\n",
    "    data.attrs['geometry'] = geom\n",
    "    data.attrs['height'] = float(matches[2][2])\n",
    "    data.attrs['offsets'] = offsets\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_dates(df):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame with a datetime index and reindexes it to include\n",
    "    all dates in the range from the minimum to the maximum date present in the index.\n",
    "    Missing dates are filled with NaN values, ensuring that the DataFrame retains its \n",
    "    original structure while providing a complete date range.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame with a datetime index that may contain missing dates.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with a complete date range as its index, with NaN values \n",
    "    for any missing dates.\n",
    "    \"\"\"\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df_full = df.reindex(full_date_range)\n",
    "    df_full.name = df.name\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframes(dfs, missing_value_threshold=None, days_included=None, minimal_offset=0):\n",
    "    \"\"\"\n",
    "    Cleans the dataframes by:\n",
    "    1. Removing dataframes without any coseismic offsets in any of the 3 components (n, e, u).\n",
    "    2. Removing non-coseismic offsets from all components.\n",
    "    3. Optionally removing dataframes with excessive missing values in all 3 components.\n",
    "    4. Optionally keeping only data within a specified range of days before the first coseismic offset\n",
    "       and after the last coseismic offset, if 'days_included' is provided.\n",
    "    5. Optionally selecting only coseismic offsets with absolute values greater than 'minimal_offset'.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "    missing_value_threshold (float, optional): Percentage (0 to 1) of allowed missing values.\n",
    "                                               If exceeded, the dataframe is removed.\n",
    "    days_included (int, optional): Number of days before and after coseismic offsets to keep.\n",
    "\n",
    "    Returns:\n",
    "    list: Cleaned list of dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_dfs = []\n",
    "    components = ['N', 'E', 'U']\n",
    "    components_offsets = ['n', 'e', 'u']\n",
    "\n",
    "    for org_df in dfs:\n",
    "        \n",
    "        has_coseismic = False\n",
    "        df = add_missing_dates(org_df)\n",
    "\n",
    "        # Determine the range of coseismic offsets\n",
    "        first_coseismic_date = None\n",
    "        last_coseismic_date = None\n",
    "        \n",
    "        for comp in components_offsets:\n",
    "            filtered_offsets = []\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                if offset['coseismic'] and abs(offset['value']) >= minimal_offset:\n",
    "                    has_coseismic = True\n",
    "                    filtered_offsets.append(offset)\n",
    "                    offset_date = pd.to_datetime(offset['date'])\n",
    "                    if first_coseismic_date is None or offset_date < first_coseismic_date:\n",
    "                        first_coseismic_date = offset_date\n",
    "                    if last_coseismic_date is None or offset_date > last_coseismic_date:\n",
    "                        last_coseismic_date = offset_date\n",
    "            # Update offsets to retain only coseismic\n",
    "            df.attrs['offsets'][comp]['offsets'] = filtered_offsets\n",
    "\n",
    "        # Skip dataframe if no coseismic offsets in any component\n",
    "        if not has_coseismic:\n",
    "            continue\n",
    "\n",
    "        # Trim data to include the range around the coseismic offsets if days_included is provided\n",
    "        if first_coseismic_date and days_included is not None:\n",
    "            start_date = first_coseismic_date - pd.Timedelta(days=days_included)\n",
    "            end_date = last_coseismic_date + pd.Timedelta(days=days_included)\n",
    "            df = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "        # Check missing values for all components combined, if threshold is provided\n",
    "        if missing_value_threshold is not None:\n",
    "            total_values = sum(df[comp].size for comp in components)\n",
    "            missing_values = sum(df[comp].isna().sum() for comp in components)\n",
    "\n",
    "            missing_percentage = missing_values / total_values\n",
    "            if missing_percentage > missing_value_threshold:\n",
    "                continue  # Skip the dataframe if missing values exceed the threshold\n",
    "\n",
    "        cleaned_dfs.append(df)\n",
    "\n",
    "    return cleaned_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dfs, interpolate=True, chunk_size=21):\n",
    "    \"\"\"\n",
    "    Extracts relevant features from a list of dataframes, including displacement values, \n",
    "    errors, offsets, decay information, station locations, and heights.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "    interpolate (bool): Whether to interpolate missing values or retain `None`.\n",
    "    chunk_size (int): Number of consecutive days to combine into one sample (row).\n",
    "\n",
    "    Returns:\n",
    "    Tuple (DataFrame, list): Combined dataframe with extracted features, and the target vector.\n",
    "    \"\"\"\n",
    "    feature_matrix = []\n",
    "    target_vector = []\n",
    "    components_offsets = ['n', 'e', 'u'] \n",
    "    \n",
    "    #columns to include in creating the chunks, (offset and decay not really necessary, as crucial information already present in labels -> pay attention to not use this information in test data)\n",
    "    #available: ['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU', 'latitude', 'longitude', 'height', 'offset_value', 'offset_error', 'decay_value', 'decay_error', 'decay_tau', 'decay_type']\n",
    "    cols = ['N', 'E', 'U']\n",
    "\n",
    "    for df in dfs:\n",
    "        # First step extract all features\n",
    "        \n",
    "        # Extract basic features (displacement, errors, correlations)\n",
    "        features = df[['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']].copy()\n",
    "        \n",
    "        # Only necessary if missing_value_thershold was bigger than 0 in the clean_dataframes function\n",
    "        if interpolate:\n",
    "            features.interpolate(method='time', inplace=True)\n",
    "\n",
    "        # Get station location and height information\n",
    "        location = df.attrs.get('geometry')\n",
    "        latitude, longitude = location.y, location.x\n",
    "        height = df.attrs.get('height')\n",
    "\n",
    "        # Extract offsets and decay information for each component\n",
    "        for comp in components_offsets:\n",
    "            series_names = ['offset_value', 'offset_error', 'decay_value', 'decay_error', 'decay_tau', 'decay_type']\n",
    "            series_dict = {name: pd.Series(0.0 if interpolate else None, dtype='float64', index=df.index) for name in series_names}\n",
    "            series_dict['decay_type'] = pd.Series(0, dtype='int64', index=df.index)\n",
    "\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                series_dict['offset_value'].loc[offset['date']] = offset['value']\n",
    "                series_dict['offset_error'].loc[offset['date']] = offset['error']\n",
    "\n",
    "            for decay in df.attrs['offsets'][comp]['ps_decays']:\n",
    "                series_dict['decay_value'].loc[decay['date']] = decay['value']\n",
    "                series_dict['decay_error'].loc[decay['date']] = decay['error']\n",
    "                series_dict['decay_tau'].loc[decay['date']] = decay['tau']\n",
    "                series_dict['decay_type'].loc[decay['date']] = 1 if decay['type'] == 'logarithmic' else 2\n",
    "\n",
    "            # Add series to features\n",
    "            for name, series in series_dict.items():\n",
    "                features[f'{comp}_{name}'] = series\n",
    "\n",
    "        # Add station metadata (location and height)\n",
    "        features['latitude'] = latitude\n",
    "        features['longitude'] = longitude\n",
    "        features['height'] = height\n",
    "\n",
    "        # Create the feature matrix with chunking -> chunks are only created for the columns that were specified earlier in the cols variable\n",
    "        for i in range(len(features) - chunk_size + 1):\n",
    "            # Create a chunk of size `chunk_size` for each feature\n",
    "            feature_row = np.hstack([features[col].values[i:i + chunk_size] for col in cols])\n",
    "            feature_matrix.append(feature_row)\n",
    "\n",
    "            # Determine the target value for this chunk: 1 if earthquake happened in chunk, 0 otherwise\n",
    "            offset_values_chunk = features[['n_offset_value', 'e_offset_value', 'u_offset_value']].iloc[i:i + chunk_size]\n",
    "            if (offset_values_chunk != 0).any().any():\n",
    "                target_vector.append(1)\n",
    "            else:\n",
    "                target_vector.append(0)\n",
    "\n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    return pd.DataFrame(feature_matrix), target_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(test_predictions):\n",
    "    \"\"\"\n",
    "    Saves the test predictions to a CSV file.\n",
    "\n",
    "    This function takes a DataFrame containing the predictions made on the test dataset\n",
    "    and saves it to a specified path defined by the PREDICTIONS_PATH variable.\n",
    "\n",
    "    Parameters:\n",
    "    test_predictions (DataFrame): The DataFrame containing the predictions to be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    test_predictions.to_csv(PREDICTIONS_PATH, index=True)\n",
    "    \n",
    "def compute_weights(train_labels):\n",
    "    \"\"\"\n",
    "    Computes class weights for handling imbalanced classes in the training dataset.\n",
    "\n",
    "    This function calculates the weights for each class in the training labels to address \n",
    "    class imbalance. The weights are computed using a balanced scheme, which assigns a \n",
    "    larger weight to underrepresented classes and a smaller weight to overrepresented ones.\n",
    "\n",
    "    Parameters:\n",
    "    train_labels (array-like): The target labels for the training dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping each class to its corresponding weight.\n",
    "    \"\"\"\n",
    "    classes = np.unique(train_labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "    return {c: w for c, w in zip(classes, class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y, test_size=0.3, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Prepares training and testing data by splitting the feature matrix and target vector.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix (chunked GNSS data).\n",
    "    y (list or Series): The target labels (0 for no offset, 1 for offset).\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    X_train (DataFrame): Training set feature matrix.\n",
    "    X_test (DataFrame): Test set feature matrix.\n",
    "    y_train (Series): Training set target vector.\n",
    "    y_test (Series): Test set target vector.\n",
    "    class_weights (dict): Weights for handling imbalanced classes.\n",
    "    \"\"\"\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    y_train = pd.Series(y_train)\n",
    "    y_test = pd.Series(y_test)\n",
    "\n",
    "    # Compute class weights to handle imbalanced dataset\n",
    "    class_weights = compute_weights(y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest():\n",
    "    \"\"\"\n",
    "    Returns a Random Forest classifier model configured with the optimal parameters.\n",
    "\n",
    "    This function initializes a RandomForestClassifier using pre-defined optimal \n",
    "    parameters stored in the BEST_PARAMS_RANDOM_FOREST variable. These parameters \n",
    "    are expected to be set prior to calling this function. \n",
    "\n",
    "    Returns:\n",
    "    RandomForestClassifier: A Random Forest model with optimal settings.\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=BEST_PARAMS_RANDOM_FOREST['n_estimators'],\n",
    "        max_depth=BEST_PARAMS_RANDOM_FOREST['max_depth'],\n",
    "        class_weight=BEST_PARAMS_RANDOM_FOREST['class_weight'],\n",
    "        random_state=BEST_PARAMS_RANDOM_FOREST['random_state']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def isolation_forest():\n",
    "    \"\"\"\n",
    "    Returns an Isolation Forest model configured with the optimal parameters.\n",
    "\n",
    "    This function initializes an IsolationForest using pre-defined optimal \n",
    "    parameters stored in the BEST_PARAMS_ISOLATION_FOREST variable. These parameters \n",
    "    are expected to be set prior to calling this function.\n",
    "\n",
    "    Returns:\n",
    "    IsolationForest: An Isolation Forest model with optimal settings.\n",
    "    \"\"\"\n",
    "    model = IsolationForest(\n",
    "        n_estimators=BEST_PARAMS_ISOLATION_FOREST['n_estimators'],\n",
    "        max_samples=BEST_PARAMS_ISOLATION_FOREST['max_samples'],\n",
    "        contamination=BEST_PARAMS_ISOLATION_FOREST['contamination'],\n",
    "        random_state=BEST_PARAMS_ISOLATION_FOREST['random_state']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def hist_gradient_boosting():\n",
    "    \"\"\"\n",
    "    Returns a HistGradientBoostingClassifier model configured with the optimal parameters.\n",
    "\n",
    "    This function initializes a HistGradientBoostingClassifier using pre-defined \n",
    "    optimal parameters stored in the BEST_PARAMS_HIST_GRADIENT_BOOSTING variable. \n",
    "    These parameters are expected to be set prior to calling this function.\n",
    "\n",
    "    Returns:\n",
    "    HistGradientBoostingClassifier: A HistGradientBoosting model with optimal settings.\n",
    "    \"\"\"\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        learning_rate=BEST_PARAMS_HIST_GRADIENT_BOOSTING['learning_rate'],\n",
    "        max_iter=BEST_PARAMS_HIST_GRADIENT_BOOSTING['max_iter'],\n",
    "        max_depth=BEST_PARAMS_HIST_GRADIENT_BOOSTING['max_depth'],\n",
    "        class_weight=BEST_PARAMS_HIST_GRADIENT_BOOSTING['class_weight'],\n",
    "        random_state=BEST_PARAMS_HIST_GRADIENT_BOOSTING['random_state'],\n",
    "        early_stopping=BEST_PARAMS_HIST_GRADIENT_BOOSTING['early_stopping'],\n",
    "        n_iter_no_change=BEST_PARAMS_HIST_GRADIENT_BOOSTING['n_iter_no_change'],\n",
    "        validation_fraction=BEST_PARAMS_HIST_GRADIENT_BOOSTING['validation_fraction']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest(X_train, y_train, weights):\n",
    "    \"\"\"\n",
    "    Optimizes a Random Forest classifier using grid search with cross-validation.\n",
    "\n",
    "    The function first checks if a pre-trained model should be loaded based on the \n",
    "    LOAD_MODEL flag. If this flag is set to True, it will load a model from the \n",
    "    specified MODEL_PATH. If the OPTIMAL_PARAMS flag is True, it will use pre-defined \n",
    "    optimal parameters for training. Otherwise, it will perform grid search to \n",
    "    identify the best hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): The training set feature matrix.\n",
    "    y_train (Series): The training set target vector (0 for no offset, 1 for offset).\n",
    "    weights (dict): Class weights to handle imbalanced classes.\n",
    "\n",
    "    Returns:\n",
    "    RandomForestClassifier: The best Random Forest model after optimization.\n",
    "    \"\"\"\n",
    "    if LOAD_MODEL:\n",
    "        print(F'Loading Random Forest model from: {MODEL_PATH}')\n",
    "        return joblib.load(MODEL_PATH)\n",
    "    \n",
    "    if OPTIMAL_PARAMS:\n",
    "        print(f'Training Random Forest model using the specified optimal parameters: {BEST_PARAMS_RANDOM_FOREST}')\n",
    "        return random_forest()\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 500],\n",
    "        'max_depth': [10, 30],\n",
    "        'class_weight': [weights], \n",
    "        'random_state': [RANDOM_STATE]\n",
    "    }\n",
    "    rf = RandomForestClassifier()\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=stratified_cv, scoring='f1_weighted', verbose=3, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best RandomForest parameters found: \", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def optimize_isolation_forest(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optimizes an Isolation Forest model for anomaly detection using grid search with cross-validation.\n",
    "\n",
    "    The function first checks if a pre-trained model should be loaded based on the \n",
    "    LOAD_MODEL flag. If this flag is set to True, it will load a model from the \n",
    "    specified MODEL_PATH. If the OPTIMAL_PARAMS flag is True, it will use pre-defined \n",
    "    optimal parameters for training. Otherwise, it will perform grid search to identify \n",
    "    the best hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): The training set feature matrix.\n",
    "    y_train (Series): The training set target vector (required for compatibility).\n",
    "\n",
    "    Returns:\n",
    "    IsolationForest: The best Isolation Forest model after optimization.\n",
    "    \"\"\"\n",
    "    if LOAD_MODEL:\n",
    "        print(F'Loading Iolation Forest model from: {MODEL_PATH}')\n",
    "        return joblib.load(MODEL_PATH)\n",
    "    \n",
    "    if OPTIMAL_PARAMS:\n",
    "        print(f'Training Isolation Forest model using the specified optimal parameters: {BEST_PARAMS_ISOLATION_FOREST}')\n",
    "        return isolation_forest()\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_samples': [0.8, 1.0],\n",
    "        'contamination': [0.001, 0.002, 0.005],\n",
    "        'random_state': [RANDOM_STATE]\n",
    "    }\n",
    "    iso_forest = IsolationForest()\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(iso_forest, param_grid, cv=stratified_cv, scoring='f1_weighted', verbose=1, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    # Have to give the labels as input even though IsolationForest uses no labels and the input is optional as otherwise there is an error. No idea why, should not impact performance though.\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best IsolationForest parameters found: \", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def optimize_hist_gradient_boosting(X_train, y_train, weights):\n",
    "    \"\"\"\n",
    "    Optimizes a HistGradientBoosting classifier using grid search with cross-validation.\n",
    "\n",
    "    The function first checks if a pre-trained model should be loaded based on the \n",
    "    LOAD_MODEL flag. If this flag is set to True, it will load a model from the \n",
    "    specified MODEL_PATH. If the OPTIMAL_PARAMS flag is True, it will use pre-defined \n",
    "    optimal parameters for training. Otherwise, it will perform grid search to identify \n",
    "    the best hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): The training set feature matrix.\n",
    "    y_train (Series): The training set target vector (0 for no offset, 1 for offset).\n",
    "    weights (dict): Class weights to handle imbalanced classes.\n",
    "\n",
    "    Returns:\n",
    "    HistGradientBoostingClassifier: The best HistGradientBoosting model after optimization.\n",
    "    \"\"\"\n",
    "    if LOAD_MODEL:\n",
    "        print(F'Loading Hist Gradient Boosting model from: {MODEL_PATH}')\n",
    "        return joblib.load(MODEL_PATH)\n",
    "    \n",
    "    if OPTIMAL_PARAMS:\n",
    "        print(f'Training Hist Gradient Boosting model using the specified optimal parameters: {BEST_PARAMS_HIST_GRADIENT_BOOSTING}')\n",
    "        return hist_gradient_boosting()\n",
    "    \n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_iter': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'class_weight': [weights],\n",
    "        'random_state': [RANDOM_STATE]\n",
    "    }\n",
    "        \n",
    "    hgb = HistGradientBoostingClassifier(early_stopping=True,\n",
    "        n_iter_no_change=7,\n",
    "        validation_fraction=0.1)\n",
    "    \n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=hgb,\n",
    "        param_grid=param_grid,\n",
    "        cv=stratified_cv,\n",
    "        scoring='f1_weighted',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best HistGradientBoosting parameters found: \", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model_type):\n",
    "    \"\"\"\n",
    "    Trains a model based on the specified type using both North, East, and Up component data.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): Feature set, including target values.\n",
    "    model_type (str): The type of model to train ('IsolationForest' or 'HistGradientBoosting').\n",
    "\n",
    "    Returns:\n",
    "    model: Trained model.\n",
    "    test_predictions: Predictions on the test set.\n",
    "    report: Classification report for the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, train_labels, test_labels, weights = prepare_data(X, y)\n",
    "\n",
    "    if model_type == 'IsolationForest':\n",
    "        model = optimize_isolation_forest(X_train, train_labels)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "    elif model_type == 'RandomForest':\n",
    "        model = optimize_random_forest(X_train, train_labels, weights)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "    \n",
    "    elif model_type == 'HistGradientBoosting':\n",
    "        model = optimize_hist_gradient_boosting(X_train, train_labels, weights)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Used Model Type not implemented. Please control spelling!')\n",
    "    \n",
    "    test_predictions = pd.Series(test_predictions, index=X_test.index)\n",
    "    \n",
    "    joblib.dump(model, MODEL_PATH)\n",
    "    save_predictions(test_predictions)\n",
    "    \n",
    "    if model_type == 'IsolationForest':\n",
    "        test_predictions = test_predictions.map({-1: 1, 1: 0})\n",
    "    report = classification_report(test_labels, test_predictions, target_names=['No Coseismic Event', 'Coseismic Event'])\n",
    "    \n",
    "    return model, test_predictions, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data processing and model training pipeline.\n",
    "\n",
    "    This function orchestrates the following steps:\n",
    "    1. Reads all data files from the specified data directory and stores them in a list.\n",
    "    2. Cleans the DataFrames using the clean_dataframes function, applying specified thresholds \n",
    "       for missing values and minimal offsets.\n",
    "    3. Extracts features and labels from the cleaned DataFrames, with an option to interpolate \n",
    "       missing data based on the selected model type.\n",
    "    4. Saves the extracted features and target labels to CSV files for further use.\n",
    "    5. Trains the specified model using the extracted features and target labels, and outputs \n",
    "       a report of the model's performance.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    dir = Path(DATA_PATH)\n",
    "    for file_path in dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            dfs.append(read_file(file_path.name))\n",
    "\n",
    "    cleaned_dfs = clean_dataframes(dfs, missing_value_threshold=0, days_included=100, minimal_offset=10)\n",
    "    \n",
    "    # HistGradientBoosting designed to deal with None data -> No interpolation needed\n",
    "    interpolate = False if MODEL_TYPE == 'HistGradientBoosting' else True\n",
    "    X, y = extract_features(cleaned_dfs, interpolate=interpolate)\n",
    "    \n",
    "    X.to_csv(f'{FEATURES_PATH}_features.csv', index=True)\n",
    "    pd.Series(y).to_csv(f'{FEATURES_PATH}_target.csv', index=False, header=False)\n",
    "    \n",
    "    model, test_predictions, report = train_model(X, y, model_type=MODEL_TYPE)\n",
    "    print(f'Report for model: {MODEL_TYPE} \\n {report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__=='__main__':\\n    main()\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
