{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import IsolationForest, HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "DATA_DIR = '/cluster/home/nteutschm/eqdetection/data/'\n",
    "RANDOM_STATE = 86\n",
    "MODEL_TYPE = 'RandomForest' # IsolationForest HistGradientBoosting RandomForest\n",
    "MODEL_PATH = f'/cluster/scratch/nteutschm/eqdetection/models/{MODEL_TYPE}.pkl'\n",
    "SAVE_PATH = f'/cluster/scratch/nteutschm/eqdetection/predictions/{MODEL_TYPE}.csv'\n",
    "STORE_FEATURES = f'/cluster/scratch/nteutschm/eqdetection/features/{MODEL_TYPE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(header_lines):\n",
    "    \"\"\"\n",
    "    Extracts offset and postseismic decay information from the header lines of a GNSS file.\n",
    "\n",
    "    The function captures both coseismic and non-coseismic offsets, along with postseismic decays, for \n",
    "    north (N), east (E), and up (U) components. It parses lines starting with '#' and collects the relevant \n",
    "    values into a structured dictionary categorized by the component.\n",
    "\n",
    "    Parameters:\n",
    "    - header_lines (list of str): Lines from the file that contain metadata and comments starting with '#'.\n",
    "\n",
    "    Returns:\n",
    "    - components (dict): A dictionary with keys 'n', 'e', and 'u' representing the north, east, and up components.\n",
    "      Each component holds a dictionary with:\n",
    "        - 'offsets': A list of dictionaries containing offset information (value, error, date, coseismic flag).\n",
    "        - 'ps_decays': A list of dictionaries containing postseismic decay information (value, error, tau, date, type).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Capture important information from the header\n",
    "    offset_pattern = re.compile(r\"#\\s*(\\*?)\\s*offset\\s+\\d+:?\\s+([-\\d.]+)\\s+\\+/\\-\\s+([-\\d.]+)\\s+mm.*?\\((\\d{4}-\\d{2}-\\d{2}).*?\\)\")\n",
    "    ps_decay_pattern = re.compile(r'#!?\\s*ps decay\\s+\\d:\\s*(-?\\d+\\.\\d+)\\s+\\+/-\\s+(\\d+\\.\\d+)\\s+mm\\s+\\((\\d{4}-\\d{2}-\\d{2})\\s+\\[(\\d{4}\\.\\d+)\\]\\);\\s*tau:\\s*(\\d+)\\s+days')\n",
    "    component_pattern = re.compile(r\"#\\s+([neu])\\s+component\")\n",
    "\n",
    "    components = {'n': {'offsets': [], 'ps_decays': []}, 'e': {'offsets': [], 'ps_decays': []}, 'u': {'offsets': [], 'ps_decays': []}}\n",
    "    current_component = None\n",
    "\n",
    "    for line in header_lines:\n",
    "        comp_match = component_pattern.match(line)\n",
    "        if comp_match:\n",
    "            current_component = comp_match.group(1)\n",
    "            continue\n",
    "\n",
    "        # Check for offset\n",
    "        offset_match = offset_pattern.match(line)\n",
    "        if offset_match and current_component:\n",
    "            coseismic = bool(offset_match.group(1))  # True if * present, meaning coseismic\n",
    "            offset_value = float(offset_match.group(2))\n",
    "            offset_error = float(offset_match.group(3))\n",
    "            offset_date = offset_match.group(4)\n",
    "            components[current_component]['offsets'].append({\n",
    "                'value': offset_value,\n",
    "                'error': offset_error,\n",
    "                'date': offset_date,\n",
    "                'coseismic': coseismic\n",
    "            })\n",
    "\n",
    "        # Check for postseismic decay\n",
    "        ps_decay_match = ps_decay_pattern.match(line)\n",
    "        if ps_decay_match and current_component:\n",
    "            decay_value = float(ps_decay_match.group(1))\n",
    "            decay_error = float(ps_decay_match.group(2))\n",
    "            decay_date = ps_decay_match.group(3)\n",
    "            tau = int(ps_decay_match.group(5))\n",
    "            # Determine decay type based on the presence of '!'\n",
    "            decay_type = 'logarithmic' if '!' in line else 'exponential'\n",
    "            components[current_component]['ps_decays'].append({\n",
    "                'value': decay_value,\n",
    "                'error': decay_error,\n",
    "                'tau': tau,\n",
    "                'date': decay_date,\n",
    "                'type': decay_type\n",
    "            })\n",
    "\n",
    "    return components\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reads a GNSS file, extracting both header and data information into a pandas DataFrame.\n",
    "\n",
    "    The function processes the header to extract metadata (e.g., station coordinates, height, offsets, decays) \n",
    "    and processes the data section to extract time-series GNSS measurements. It combines these into a DataFrame \n",
    "    with attributes containing additional metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the file containing GNSS data.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame): A DataFrame containing the time-series GNSS data (N, E, U components, sigmas, correlations),\n",
    "      indexed by date. The DataFrame has additional attributes storing station geometry (latitude, longitude), height, \n",
    "      and offset/decay information.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(DATA_DIR+filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    header_lines = [line for line in lines if line.startswith('#')]\n",
    "    if header_lines:\n",
    "        column_names = re.split(r'\\s{2,}', header_lines[-1].lstrip('#').strip())\n",
    "    else:\n",
    "        column_names = []\n",
    "        \n",
    "    data_lines = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.strip().split()\n",
    "            # Check if the number of parts matches the expected number of columns\n",
    "            if len(parts) < len(column_names):\n",
    "                # Add None for missing values\n",
    "                parts.extend([None] * (len(column_names) - len(parts)))\n",
    "            data_lines.append(parts)\n",
    "\n",
    "    data = pd.DataFrame(data_lines)\n",
    "    data.columns = column_names\n",
    "    \n",
    "    # Extracts latitude, longitude and height\n",
    "    pattern = r'Latitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|East Longitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|Height\\s*\\(M\\)\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    #referece_pattern = r'Reference_X\\s*:\\s*(-?\\d+\\.\\d+)|Reference_Y\\s*:\\s*(-?\\d+\\.\\d+)|Reference_Z\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    matches = re.findall(pattern, ' '.join(header_lines))\n",
    "    geom = Point(float(matches[1][1]), float(matches[0][0]))\n",
    "    \n",
    "    offsets = get_offsets(header_lines)\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Yr'].astype(str) + data['DayOfYr'].astype(str), format='%Y%j')\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data.drop(['Dec Yr', 'Yr', 'DayOfYr', 'Chi-Squared'], axis=1, inplace=True)\n",
    "    cols = ['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']\n",
    "    data[cols] = data[cols].astype(float)\n",
    "    \n",
    "    data.name = filename.replace(\"RawTrend.neu\", \"\")\n",
    "    data.attrs['geometry'] = geom\n",
    "    data.attrs['height'] = float(matches[2][2])\n",
    "    data.attrs['offsets'] = offsets\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_dates(df):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df_full = df.reindex(full_date_range)\n",
    "    df_full.name = df.name\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframes(dfs, missing_value_threshold=None):\n",
    "    \"\"\"\n",
    "    Cleans the dataframes by:\n",
    "    1. Removing dataframes without any coseismic offsets in any of the 3 components (n, e, u).\n",
    "    2. Removing non-coseismic offsets from all components.\n",
    "    3. Optionally removing dataframes with excessive missing values in all 3 components.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "    missing_value_threshold (float, optional): Percentage (0 to 1) of allowed missing values.\n",
    "                                               If exceeded, the dataframe is removed.\n",
    "\n",
    "    Returns:\n",
    "    list: Cleaned list of dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_dfs = []\n",
    "    components = ['N', 'E', 'U']\n",
    "    components_offsets = ['n', 'e', 'u']\n",
    "\n",
    "    for org_df in dfs:\n",
    "        \n",
    "        has_coseismic = False\n",
    "        df = add_missing_dates(org_df)\n",
    "\n",
    "        for comp in components_offsets:\n",
    "            filtered_offsets = []\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                if offset['coseismic']:\n",
    "                    has_coseismic = True\n",
    "                    filtered_offsets.append(offset)\n",
    "            # Update offsets to retain only coseismic\n",
    "            df.attrs['offsets'][comp]['offsets'] = filtered_offsets\n",
    "\n",
    "        # Skip dataframe if no coseismic offsets in any component\n",
    "        if not has_coseismic:\n",
    "            continue\n",
    "\n",
    "        # Check missing values for all components combined, if threshold is provided\n",
    "        if missing_value_threshold is not None:\n",
    "            total_values = sum(df[comp].size for comp in components)\n",
    "            missing_values = sum(df[comp].isna().sum() for comp in components)\n",
    "\n",
    "            missing_percentage = missing_values / total_values\n",
    "            if missing_percentage > missing_value_threshold:\n",
    "                continue  # Skip the dataframe if missing values exceed the threshold\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        cleaned_dfs.append(df)\n",
    "\n",
    "    return cleaned_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dfs, interpolate=True, chunk_size=21):\n",
    "    \"\"\"\n",
    "    Extracts relevant features from a list of dataframes, including displacement values, \n",
    "    errors, offsets, decay information, station locations, and heights.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "    interpolate (bool): Whether to interpolate missing values or retain `None`.\n",
    "    chunk_size (int): Number of consecutive days to combine into one sample (row).\n",
    "\n",
    "    Returns:\n",
    "    Tuple (DataFrame, list): Combined dataframe with extracted features, and the target vector.\n",
    "    \"\"\"\n",
    "    feature_matrix = []\n",
    "    target_vector = []\n",
    "    components_offsets = ['n', 'e', 'u']  # North, East, Up\n",
    "    \n",
    "    #columns to include in creating the chunks\n",
    "    #cols = ['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']\n",
    "    cols = ['N', 'E', 'U']\n",
    "\n",
    "    for df in dfs:\n",
    "        # Extract basic features (displacement, errors, correlations)\n",
    "        features = df[['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']].copy()\n",
    "\n",
    "        if interpolate:\n",
    "            features.interpolate(method='time', inplace=True)\n",
    "\n",
    "        # Get station location and height information\n",
    "        location = df.attrs.get('geometry')\n",
    "        if isinstance(location, Point):\n",
    "            latitude, longitude = location.y, location.x\n",
    "        else:\n",
    "            latitude, longitude = None, None\n",
    "        height = df.attrs.get('height', None)\n",
    "\n",
    "        # Extract offsets and decay information for each component\n",
    "        for comp in components_offsets:\n",
    "            series_names = ['offset_value', 'offset_error', 'decay_value', 'decay_error', 'decay_tau', 'decay_type']\n",
    "            series_dict = {name: pd.Series(0.0 if interpolate else None, dtype='float64', index=df.index) for name in series_names}\n",
    "            series_dict['decay_type'] = pd.Series(0.0, dtype='int64', index=df.index)  # decay_type should be int\n",
    "\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                series_dict['offset_value'].loc[offset['date']] = offset['value']\n",
    "                series_dict['offset_error'].loc[offset['date']] = offset['error']\n",
    "\n",
    "            for decay in df.attrs['offsets'][comp]['ps_decays']:\n",
    "                series_dict['decay_value'].loc[decay['date']] = decay['value']\n",
    "                series_dict['decay_error'].loc[decay['date']] = decay['error']\n",
    "                series_dict['decay_tau'].loc[decay['date']] = decay['tau']\n",
    "                series_dict['decay_type'].loc[decay['date']] = 1 if decay['type'] == 'logarithmic' else 2\n",
    "\n",
    "            # Add series to features\n",
    "            for name, series in series_dict.items():\n",
    "                features[f'{comp}_{name}'] = series\n",
    "\n",
    "        # Add station metadata (location and height)\n",
    "        features['latitude'] = latitude\n",
    "        features['longitude'] = longitude\n",
    "        features['height'] = height\n",
    "\n",
    "        # Create the feature matrix with chunking\n",
    "        for i in range(len(features) - chunk_size + 1):\n",
    "            # Create a chunk of size `chunk_size` for each feature (ignore offsets and decays as it takes too much memory (more than 80GB))\n",
    "            #feature_row = np.hstack([features[col].values[i:i + chunk_size] for col in features.columns])\n",
    "            feature_row = np.hstack([features[col].values[i:i + chunk_size] for col in cols])\n",
    "\n",
    "            # Add the row to the feature matrix\n",
    "            feature_matrix.append(feature_row)\n",
    "\n",
    "            # Determine the target value for this chunk: index of earthquake or 0 if none\n",
    "            offset_values_chunk = features[['n_offset_value', 'e_offset_value', 'u_offset_value']].iloc[i:i + chunk_size]\n",
    "            if (offset_values_chunk != 0).any().any():\n",
    "                target_vector.append(1)  # earthquake/discontinuity in this chunk\n",
    "            else:\n",
    "                target_vector.append(0)  # no discontinuity\n",
    "\n",
    "    feature_matrix = np.array(feature_matrix)\n",
    "    return pd.DataFrame(feature_matrix), target_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(test_predictions):\n",
    "    test_predictions.to_csv(SAVE_PATH, index=True)\n",
    "    \n",
    "def compute_weights(train_labels):\n",
    "    classes = np.unique(train_labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "    return {c: w for c, w in zip(classes, class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepares training and testing data by splitting the feature matrix and target vector.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix (chunked GNSS data).\n",
    "    y (list or Series): The target labels (0 for no offset, 1 for offset).\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    X_train (DataFrame): Training set feature matrix.\n",
    "    X_test (DataFrame): Test set feature matrix.\n",
    "    y_train (Series): Training set target vector.\n",
    "    y_test (Series): Test set target vector.\n",
    "    class_weights (dict): Weights for handling imbalanced classes.\n",
    "    \"\"\"\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Convert target lists to Pandas Series if necessary\n",
    "    if isinstance(y_train, list):\n",
    "        y_train = pd.Series(y_train)\n",
    "    if isinstance(y_test, list):\n",
    "        y_test = pd.Series(y_test)\n",
    "\n",
    "    # Compute class weights to handle imbalanced dataset\n",
    "    class_weights = compute_weights(y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest(X_train, y_train, weights):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 500],\n",
    "        'max_depth': [10, 30],\n",
    "        'class_weight': [weights], \n",
    "        'random_state': [RANDOM_STATE]\n",
    "    }\n",
    "    rf = RandomForestClassifier()\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=stratified_cv, scoring='f1_weighted', verbose=3, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best RandomForest parameters found: \", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def optimize_isolation_forest(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_samples': [0.8, 1.0],\n",
    "        'contamination': [0.001, 0.002, 0.005],\n",
    "        'random_state': [RANDOM_STATE]\n",
    "    }\n",
    "    iso_forest = IsolationForest()\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(iso_forest, param_grid, cv=stratified_cv, scoring='f1_weighted', verbose=1, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    # Have to give the labels as input even though IsolationForest uses no labels and the input is optional as otherwise there is an error. No idea why. \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best IsolationForest parameters found: \", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def optimize_hist_gradient_boosting(X_train, y_train, weights):\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_iter': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'class_weight': [weights],\n",
    "        'random_state': [RANDOM_STATE]\n",
    "    }\n",
    "        \n",
    "    hgb = HistGradientBoostingClassifier()\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=hgb,\n",
    "        param_grid=param_grid,\n",
    "        cv=stratified_cv,\n",
    "        scoring='f1_weighted',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best HistGradientBoosting parameters found: \", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model_type):\n",
    "    \"\"\"\n",
    "    Trains a model based on the specified type using both North, East, and Up component data.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): Feature set, including target values.\n",
    "    model_type (str): The type of model to train ('IsolationForest' or 'HistGradientBoosting').\n",
    "\n",
    "    Returns:\n",
    "    model: Trained model.\n",
    "    test_predictions: Predictions on the test set.\n",
    "    report: Classification report for the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, train_labels, test_labels, weights = prepare_data(X, y)\n",
    "    \n",
    "    # Imbalance test: No offsets: 2133084, offsets: 525\n",
    "\n",
    "    # Set offset/decay columns based on model type\n",
    "    if model_type == 'IsolationForest':\n",
    "        # For IsolationForest, ignore decays and offset information (unsupervised)\n",
    "        #columns = [col for col in X_test.columns if 'offset' in col or 'decay' in col]\n",
    "        #X_test.drop(columns, axis=1, inplace=True)\n",
    "        #X_train.drop(columns, axis=1, inplace=True)\n",
    "        model = optimize_isolation_forest(X_train, train_labels)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "    elif model_type == 'RandomForest':\n",
    "        # For RandomForest, ignore decays and offset information\n",
    "        #columns = [col for col in X_test.columns if 'offset' in col or 'decay' in col]\n",
    "        #X_test.drop(columns, axis=1, inplace=True)\n",
    "        #X_train.drop(columns, axis=1, inplace=True)\n",
    "        model = optimize_random_forest(X_train, train_labels, weights)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "    \n",
    "    elif model_type == 'HistGradientBoosting':\n",
    "        # For HistGradientBoosting, set offset and decay columns to None in test data (supervised using binary training labels)\n",
    "        #X_test[[col for col in X_test.columns if 'offset' in col or 'decay' in col]] = None\n",
    "        model = optimize_hist_gradient_boosting(X_train, train_labels, weights)\n",
    "        test_predictions = model.predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Used Model Type not implement. Please control spelling!')\n",
    "    \n",
    "    test_predictions = pd.Series(test_predictions, index=X_test.index)\n",
    "    \n",
    "    joblib.dump(model, MODEL_PATH)\n",
    "    save_predictions(test_predictions)\n",
    "    \n",
    "    if model_type == 'IsolationForest':\n",
    "        test_predictions = test_predictions.map({-1: 1, 1: 0})\n",
    "    report = classification_report(test_labels, test_predictions, target_names=['No Coseismic Event', 'Coseismic Event'])\n",
    "    \n",
    "    print(f\"Class weights used during training:\\nNo Coseismic Event: {weights[0]} \\nCoseismic Event: {weights[1]}\")\n",
    "    \n",
    "    return model, test_predictions, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dfs = []\n",
    "    dir = Path(DATA_DIR)\n",
    "    for file_path in dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            dfs.append(read_file(file_path.name))\n",
    "\n",
    "    cleaned_dfs = clean_dataframes(dfs, missing_value_threshold=0.05)\n",
    "    \n",
    "    # HistGradientBoosting designed to deal with None data -> No interpolation needed\n",
    "    interpolate = False if MODEL_TYPE == 'HistGradientBoosting' else True\n",
    "    X, y = extract_features(cleaned_dfs, interpolate=interpolate)\n",
    "    \n",
    "    X.to_csv(f'{STORE_FEATURES}_features.csv', index=True)\n",
    "    pd.Series(y).to_csv(f'{STORE_FEATURES}_target.csv', index=False, header=False)\n",
    "    \n",
    "    model, test_predictions, report = train_model(X, y, model_type=MODEL_TYPE)\n",
    "    print(f'Report for model: {MODEL_TYPE} \\n {report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__=='__main__':\\n    main()\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
