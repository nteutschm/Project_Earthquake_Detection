{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "\n",
    "DATA_DIR = '/cluster/home/nteutschm/eqdetection/data/'\n",
    "#DATA_DIR = 'Data/'\n",
    "RANDOM_STATE = 86\n",
    "MODEL_PATH = '/cluster/scratch/nteutschm/eqdetection/models/'\n",
    "SAVE_PATH = '/cluster/scratch/nteutschm/eqdetection/predictions/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets(header_lines):\n",
    "    # Capture important information from the header\n",
    "    offset_pattern = re.compile(r\"#\\s*(\\*?)\\s*offset\\s+\\d+:?\\s+([-\\d.]+)\\s+\\+/\\-\\s+([-\\d.]+)\\s+mm.*?\\((\\d{4}-\\d{2}-\\d{2}).*?\\)\")\n",
    "    ps_decay_pattern = re.compile(r'#!?\\s*ps decay\\s+\\d:\\s*(-?\\d+\\.\\d+)\\s+\\+/-\\s+(\\d+\\.\\d+)\\s+mm\\s+\\((\\d{4}-\\d{2}-\\d{2})\\s+\\[(\\d{4}\\.\\d+)\\]\\);\\s*tau:\\s*(\\d+)\\s+days')\n",
    "    component_pattern = re.compile(r\"#\\s+([neu])\\s+component\")\n",
    "\n",
    "    components = {'n': {'offsets': [], 'ps_decays': []}, 'e': {'offsets': [], 'ps_decays': []}, 'u': {'offsets': [], 'ps_decays': []}}\n",
    "    current_component = None\n",
    "\n",
    "    for line in header_lines:\n",
    "        comp_match = component_pattern.match(line)\n",
    "        if comp_match:\n",
    "            current_component = comp_match.group(1)\n",
    "            continue\n",
    "\n",
    "        # Check for offset\n",
    "        offset_match = offset_pattern.match(line)\n",
    "        if offset_match and current_component:\n",
    "            coseismic = bool(offset_match.group(1))  # True if * present, meaning coseismic\n",
    "            offset_value = float(offset_match.group(2))\n",
    "            offset_error = float(offset_match.group(3))\n",
    "            offset_date = offset_match.group(4)\n",
    "            components[current_component]['offsets'].append({\n",
    "                'value': offset_value,\n",
    "                'error': offset_error,\n",
    "                'date': offset_date,\n",
    "                'coseismic': coseismic\n",
    "            })\n",
    "\n",
    "        # Check for postseismic decay\n",
    "        ps_decay_match = ps_decay_pattern.match(line)\n",
    "        if ps_decay_match and current_component:\n",
    "            decay_value = float(ps_decay_match.group(1))\n",
    "            decay_error = float(ps_decay_match.group(2))\n",
    "            decay_date = ps_decay_match.group(3)\n",
    "            tau = int(ps_decay_match.group(5))\n",
    "            # Determine decay type based on the presence of '!'\n",
    "            decay_type = 'logarithmic' if '!' in line else 'exponential'\n",
    "            components[current_component]['ps_decays'].append({\n",
    "                'value': decay_value,\n",
    "                'error': decay_error,\n",
    "                'tau': tau,\n",
    "                'date': decay_date,\n",
    "                'type': decay_type\n",
    "            })\n",
    "\n",
    "    return components\n",
    "\n",
    "def read_file(filename):\n",
    "    \n",
    "    with open(DATA_DIR+filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    header_lines = [line for line in lines if line.startswith('#')]\n",
    "    if header_lines:\n",
    "        column_names = re.split(r'\\s{2,}', header_lines[-1].lstrip('#').strip())\n",
    "    else:\n",
    "        column_names = []\n",
    "        \n",
    "    data_lines = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.strip().split()\n",
    "            # Check if the number of parts matches the expected number of columns\n",
    "            if len(parts) < len(column_names):\n",
    "                # Add None for missing values\n",
    "                parts.extend([None] * (len(column_names) - len(parts)))\n",
    "            data_lines.append(parts)\n",
    "\n",
    "    data = pd.DataFrame(data_lines)\n",
    "    data.columns = column_names\n",
    "    \n",
    "    # Extracts latitude, longitude and height\n",
    "    pattern = r'Latitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|East Longitude\\(DD\\)\\s*:\\s*(-?\\d+\\.\\d+)|Height\\s*\\(M\\)\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    #referece_pattern = r'Reference_X\\s*:\\s*(-?\\d+\\.\\d+)|Reference_Y\\s*:\\s*(-?\\d+\\.\\d+)|Reference_Z\\s*:\\s*(-?\\d+\\.\\d+)'\n",
    "    matches = re.findall(pattern, ' '.join(header_lines))\n",
    "    geom = Point(float(matches[1][1]), float(matches[0][0]))\n",
    "    \n",
    "    offsets = get_offsets(header_lines)\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Yr'].astype(str) + data['DayOfYr'].astype(str), format='%Y%j')\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data.drop(['Dec Yr', 'Yr', 'DayOfYr', 'Chi-Squared'], axis=1, inplace=True)\n",
    "    cols = ['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']\n",
    "    data[cols] = data[cols].astype(float)\n",
    "    \n",
    "    data.name = filename.replace(\"RawTrend.neu\", \"\")\n",
    "    data.attrs['geometry'] = geom\n",
    "    data.attrs['height'] = float(matches[2][2])\n",
    "    data.attrs['offsets'] = offsets\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_files():\n",
    "    gdfs = []\n",
    "    dir = Path(DATA_DIR)\n",
    "    for file_path in dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            gdfs.append(read_file(file_path.name))\n",
    "    return gdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_dates(df):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    full_date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df_full = df.reindex(full_date_range)\n",
    "    df_full.name = df.name\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframes(dfs, missing_value_threshold=None):\n",
    "    \"\"\"\n",
    "    Cleans the dataframes by:\n",
    "    1. Removing dataframes without any coseismic offsets in any of the 3 components (n, e, u).\n",
    "    2. Removing non-coseismic offsets from all components.\n",
    "    3. Optionally removing dataframes with excessive missing values in all 3 components.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "    missing_value_threshold (float, optional): Percentage (0 to 1) of allowed missing values.\n",
    "                                               If exceeded, the dataframe is removed.\n",
    "\n",
    "    Returns:\n",
    "    list: Cleaned list of dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_dfs = []\n",
    "    components = ['N', 'E', 'U']\n",
    "    components_offsets = ['n', 'e', 'u']\n",
    "\n",
    "    for org_df in dfs:\n",
    "        \n",
    "        has_coseismic = False\n",
    "        df = add_missing_dates(org_df)\n",
    "\n",
    "        for comp in components_offsets:\n",
    "            filtered_offsets = []\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                if offset['coseismic']:\n",
    "                    has_coseismic = True\n",
    "                    filtered_offsets.append(offset)\n",
    "            # Update offsets to retain only coseismic\n",
    "            df.attrs['offsets'][comp]['offsets'] = filtered_offsets\n",
    "\n",
    "        # Skip dataframe if no coseismic offsets in any component\n",
    "        if not has_coseismic:\n",
    "            continue\n",
    "\n",
    "        # Check missing values for all components combined, if threshold is provided\n",
    "        if missing_value_threshold is not None:\n",
    "            total_values = sum(df[comp].size for comp in components)\n",
    "            missing_values = sum(df[comp].isna().sum() for comp in components)\n",
    "\n",
    "            missing_percentage = missing_values / total_values\n",
    "            if missing_percentage > missing_value_threshold:\n",
    "                continue  # Skip the dataframe if missing values exceed the threshold\n",
    "\n",
    "        # Add the cleaned dataframe to the list\n",
    "        cleaned_dfs.append(df)\n",
    "\n",
    "    return cleaned_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dfs):\n",
    "    \"\"\"\n",
    "    Extracts relevant features from a list of dataframes, including displacement values, \n",
    "    errors, offsets, decay information, station locations, and heights.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list): List of dataframes with GNSS data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Combined dataframe with all extracted features.\n",
    "    \"\"\"\n",
    "    feature_dfs = []\n",
    "    components_offsets = ['n', 'e', 'u']\n",
    "\n",
    "    for df in dfs:\n",
    "        features = df[['N', 'E', 'U', 'N sig', 'E sig', 'U sig', 'CorrNE', 'CorrNU', 'CorrEU']].copy()\n",
    "        \n",
    "        features.interpolate(method='time', inplace=True)\n",
    "\n",
    "        location = df.attrs.get('geometry')\n",
    "        if isinstance(location, Point):\n",
    "            features['latitude'] = location.y\n",
    "            features['longitude'] = location.x\n",
    "\n",
    "        features['height'] = df.attrs.get('height')\n",
    "        \n",
    "        for comp in components_offsets:\n",
    "            offset_series = pd.Series(0, index=df.index)\n",
    "            offset_error_series = pd.Series(0, index=df.index)\n",
    "\n",
    "            for offset in df.attrs['offsets'][comp]['offsets']:\n",
    "                offset_series.loc[offset['date']] = offset['value']\n",
    "                offset_error_series.loc[offset['date']] = offset['error']\n",
    "\n",
    "            features[f'{comp}_offset_value'] = offset_series\n",
    "            features[f'{comp}_offset_error'] = offset_error_series\n",
    "        \n",
    "        for comp in components_offsets:\n",
    "            decay_value_series = pd.Series(0, index=df.index)\n",
    "            decay_tau_series = pd.Series(0, index=df.index)\n",
    "            decay_error_series = pd.Series(0, index=df.index)\n",
    "            decay_type_series = pd.Series(0, index=df.index)\n",
    "\n",
    "            for decay in df.attrs['offsets'][comp]['ps_decays']:\n",
    "                decay_value_series.loc[decay['date']] = decay['value']\n",
    "                decay_error_series.loc[decay['date']] = decay['error']\n",
    "                decay_tau_series.loc[decay['date']] = decay['tau']\n",
    "                decay_type_series.loc[decay['date']] = 1 if decay['type'] == 'logarithmic' else 2 if decay['type'] == 'exponential' else 0\n",
    "            features[f'{comp}_decay_value'] = decay_value_series\n",
    "            features[f'{comp}_decay_error'] = decay_error_series\n",
    "            features[f'{comp}_decay_tau'] = decay_tau_series\n",
    "            features[f'{comp}_decay_type'] = decay_type_series\n",
    "        \n",
    "        feature_dfs.append(features)\n",
    "    \n",
    "    return feature_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_isolation_forest(X):\n",
    "    X_train, y_test = train_test_split(X, test_size=0.3, random_state=RANDOM_STATE)\n",
    "    y_test = pd.concat(y_test, axis=0)\n",
    "    X_test = y_test.drop(columns=[col for col in y_test.columns if 'offset' in col or 'decay' in col])\n",
    "    \n",
    "    model = IsolationForest(contamination=0.05, random_state=RANDOM_STATE)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    joblib.dump(model, MODEL_PATH)\n",
    "    \n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    return model, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(test_predictions):\n",
    "    test_predictions_df = pd.DataFrame(test_predictions, columns=['Predictions'])\n",
    "    test_predictions_df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dfs = organize_files()\n",
    "    cleaned_dfs = clean_dataframes(dfs, missing_value_threshold=0.2)\n",
    "    X = extract_features(cleaned_dfs)\n",
    "    model, test_predictions = train_isolation_forest(X)\n",
    "    save_predictions(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
